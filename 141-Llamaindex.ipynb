{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT-Web/blob/main/141-Llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf01a75-a01b-472e-bc0c-9fe97658eb46",
      "metadata": {
        "id": "3bf01a75-a01b-472e-bc0c-9fe97658eb46"
      },
      "source": [
        "## LlamaIndex <> Langchain Integrations\n",
        "\n",
        "This demo notebook shows how you can provide integrations between LlamaIndex and Langchain. It provides the following examples:\n",
        "- Using LlamaIndex as a callable tool with a Langchain agent\n",
        "- Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "Ol6-Nyqwk86O",
        "outputId": "54b94885-8a3c-4776-8d2b-d7be9325a298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ol6-Nyqwk86O",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index pypdf"
      ],
      "metadata": {
        "id": "Qu8kWA3dlPH7"
      },
      "id": "Qu8kWA3dlPH7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community"
      ],
      "metadata": {
        "id": "rCOGQ8iFlYhE"
      },
      "id": "rCOGQ8iFlYhE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "ok3aHrKyNNPh",
        "outputId": "7f88ff92-0bdf-4ddb-e007-9e95ccccef9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ok3aHrKyNNPh",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=sk-\" >> .env"
      ],
      "metadata": {
        "id": "8Zwd9XfDNJzm"
      },
      "id": "8Zwd9XfDNJzm",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai, os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "# Access the API key using the variable name defined in the .env file\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "zYrBeA8GNDoO"
      },
      "id": "zYrBeA8GNDoO",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1568569",
      "metadata": {
        "id": "c1568569"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb9177a4-9cb7-4211-b463-121d850b5917",
      "metadata": {
        "id": "bb9177a4-9cb7-4211-b463-121d850b5917"
      },
      "source": [
        "#### Using LlamaIndex as a Callable Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "12af1b0e-983f-4fc1-b5b4-2edeb2e8f07e",
      "metadata": {
        "id": "12af1b0e-983f-4fc1-b5b4-2edeb2e8f07e"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "KCwlcBbUmA26"
      },
      "id": "KCwlcBbUmA26",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1226132b-2c5f-4073-bfdb-0e36c681c12f",
      "metadata": {
        "id": "1226132b-2c5f-4073-bfdb-0e36c681c12f"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8c9b3567-c95c-473d-afc0-516b5f35e197",
      "metadata": {
        "id": "8c9b3567-c95c-473d-afc0-516b5f35e197"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"LlamaIndex\",\n",
        "        func=lambda q: str(index.as_query_engine().query(q)),\n",
        "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
        "        return_direct=True,\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "374e9f69-1f75-4a62-afdc-22f748d4bddd",
      "metadata": {
        "id": "374e9f69-1f75-4a62-afdc-22f748d4bddd",
        "outputId": "5ee53235-7199-463b-98fc-f4b73e3fc16d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "agent_executor = initialize_agent(\n",
        "    tools, llm, agent=\"conversational-react-description\", memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "579fbc9f-9f13-416c-bde4-7e56fb899727",
      "metadata": {
        "id": "579fbc9f-9f13-416c-bde4-7e56fb899727",
        "outputId": "33cdb3a7-183c-4944-e572-ece59fad88e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Bob! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "agent_executor.run(input=\"hi, i am bob\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9841c8e-f90b-4e40-a2f9-ad1e98bb9eef",
      "metadata": {
        "id": "a9841c8e-f90b-4e40-a2f9-ad1e98bb9eef"
      },
      "outputs": [],
      "source": [
        "agent_executor.run(input=\"what's my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c02eb88-5a4a-4694-9b77-cd46adc691f5",
      "metadata": {
        "id": "7c02eb88-5a4a-4694-9b77-cd46adc691f5"
      },
      "source": [
        "#### Using LlamaIndex as a memory module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06a04c1-c5fa-482c-b4d7-9b3fa0f904af",
      "metadata": {
        "id": "e06a04c1-c5fa-482c-b4d7-9b3fa0f904af"
      },
      "outputs": [],
      "source": [
        "# try using SummaryIndex!\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.llms import OpenAIChat\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e6694b-25fc-4fbc-8223-9a5605dc641f",
      "metadata": {
        "id": "00e6694b-25fc-4fbc-8223-9a5605dc641f"
      },
      "outputs": [],
      "source": [
        "index = SummaryIndex([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c0b10c-bca4-49f1-9353-646a182050cf",
      "metadata": {
        "id": "25c0b10c-bca4-49f1-9353-646a182050cf"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "# NOTE: you can also use a conversational chain\n",
        "\n",
        "memory = GPTIndexChatMemory(\n",
        "    index=index,\n",
        "    memory_key=\"chat_history\",\n",
        "    query_kwargs={\"response_mode\": \"compact\"},\n",
        "    # return_source returns source nodes instead of querying index\n",
        "    return_source=True,\n",
        "    # return_messages returns context in message format\n",
        "    return_messages=True,\n",
        ")\n",
        "# llm = OpenAIChat(temperature=0)\n",
        "llm=OpenAI(temperature=0)\n",
        "agent_executor = initialize_agent(\n",
        "    [], llm, agent=\"conversational-react-description\", memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76193275-c47c-426c-b7e4-c54d31fda92d",
      "metadata": {
        "id": "76193275-c47c-426c-b7e4-c54d31fda92d",
        "outputId": "928282ee-d743-42b2-8a7d-52525a4e88bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Bob, nice to meet you! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "agent_executor.run(input=\"hi, i am bob\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d426239a-a38b-4ae9-838b-b6fab43970e0",
      "metadata": {
        "id": "d426239a-a38b-4ae9-838b-b6fab43970e0",
        "outputId": "b5158371-41b9-4f67-9029-3a09270e6ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is your name?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# NOTE: the query now calls the SummaryIndex memory module.\n",
        "agent_executor.run(input=\"what's my name?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}